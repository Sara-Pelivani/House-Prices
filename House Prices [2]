#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Dec 17 18:48:33 2019

@author: sarapelivani
"""

#Import important libraries 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn import linear_model

#Use Pandas to read csv files 
df=pd.read_csv(r'/Users/sarapelivani/Desktop/House Prices /house-prices-advanced-regression-techniques/train.csv')
test_df=pd.read_csv(r'/Users/sarapelivani/Desktop/House Prices /house-prices-advanced-regression-techniques/test.csv')

print('Train data shape:',df.shape)
print('Test data shape:',test_df.shape)
#Test obviously has one less column as does not include the final sell price info

df.head()

plt.style.use(style='ggplot')
plt.rcParams['figure.figsize']=(10,6)

#Getting info on Sales Prices
print(df.SalePrice.describe())

#Check for skewness, i.e. measure of shape of the distribution of values
print('Skew is:', df.SalePrice.skew())
plt.hist(df.SalePrice, color='blue')
plt.show()

#Distribution is positively skewed

#Log-transform the traget variable. This is to imporve linearity of the data.
#The predictions generated by the final model will also be log-transformed, so we'll have to put these back to their original form later

target=np.log(df.SalePrice)
print("\n Skew is:", target.skew())
plt.hist(target, color='blue')
plt.show()

#'Skew' value closer to zero, which is better. I.e. skewness is improved. Now we have a more normal distribution

#Starting Feature Engineering:
 
#Investigating the relationship bewtween columns. Note, any NAN values is automatically excluded.

print('\n')

numeric_features = df.select_dtypes(include=[np.number])
corr = numeric_features.corr()

#Note, only looking at 5 most positively correlated and 5 most negatively correlated
print(corr['SalePrice'].sort_values(ascending=False)[:5], '\n')
print(corr['SalePrice'].sort_values(ascending=False)[-5:],)
 
#Can see that the first 5 features are positively correlated with Sales Price
#Next 5 features are negatively correlated 

#Using scatter plot to see relationship between Garage Area and Sale Price

plt.scatter(x=df['GarageArea'], y=target)
plt.ylabel('Sale Price')
plt.xlabel('Garage Area')
plt.show()

#Can see many houses sold with no Garage at all
#Also want to remove outlies present at very large Garage areas
#Create new dataframe with outliers removed

df=df[df['GarageArea']<1200]

plt.scatter(x=df['GarageArea'], y=np.log(df.SalePrice))
plt.xlim(-200,1600)
plt.ylabel('Sale Price')
plt.xlabel('Garage Area')
plt.show()

#Handling null values;

nulls = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)[:25])
nulls.columns = ['Null Count']
nulls.index.name = 'Feature'
print(nulls)

#Looking at non-numeric features 
categoricals = df.select_dtypes(exclude=[np.number])
print(categoricals.describe())

#Will use one-hot encoding to transform categorical data into numerical data
#Need to make sure any transformation applied to the train dataset is aslo applied to the test dataset

#Will transform the Street column onto a Boolean column;
print('Original: \n')
print (df.Street.value_counts(), '\n')
df['enc_street']=pd.get_dummies(df.Street, drop_first=True)
test_df['enc_street']=pd.get_dummies(test_df.Street, drop_first=True)
print('Encoded: \n')
print(df.enc_street.value_counts())
#Pave and Grvl converted to 1 and 0

#Look at SaleConidtion by making a pivot table
condition_pivot=df.pivot_table(index='SaleCondition', values='SalePrice',
aggfunc=np.median)
condition_pivot.plot(kind='bar', color='blue')
plt.xlabel('Sale Condition')
plt.ylabel('Median Sale Price')
plt.xticks(rotation=0)
plt.show()

#Partial has much higher median sale price than the others.
#Will encode this in a new feature. Will set all 'partial' values to 1, while all other values to 0
  
def encode(x):return 1 if x=='Partial' else 0
df['enc_condition']=df.SaleCondition.apply(encode)
test_df['enc_condition']=test_df.SaleCondition.apply(encode)
 
condition_pivot=df.pivot_table(index='enc_condition', values='SalePrice',
aggfunc=np.median)
condition_pivot.plot(kind='bar', color='blue')
plt.xlabel('Encoded Sale Condition')
plt.ylabel('Median Sale Price')
plt.xticks(rotation=0)
plt.show() 

#Filling in missing values
#Will use method of interpolation, i.e. filling in missing values with an average value and then assign the results to data

data=df.select_dtypes(include=[np.number]).interpolate().dropna()
print(sum(data.isnull().sum() != 0)) 

#Now no null values

#Building Linear Model
#Seperate features and target variables for modeling. Features to X and target variables to Y

Y = np.log(df.SalePrice)
X = data.drop(['SalePrice', 'Id'], axis=1)

#Partition of Data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42, test_size=33)
 
#Linear Regression Model
lr=linear_model.LinearRegression()

#need to preform some model fitting
#used to estimate relationship between predictors and target variablesso we can make accurate predictions on new data
#will fit moedel using x_train and y_train
#will score with x_test and y_test

model=lr.fit(X_train, Y_train)

#finging r-squared value (measure of how close the data are to the fitted regression line)
#1 means that all variance in the target is explained by the data
#0 means opposite
print("R^2 is: \n", model.score(X_test, Y_test))
#is roughly 83% in my target variables 

#want to find RMSE (root mean square error)
#use model to make predictions on test data set
predictions=model.predict(X_test)
print('RMSE is: \n', mean_squared_error(Y_test, predictions))

#RMSE measures the distance between the predicted values and actual values 
#can be viewed graphically 

actual_values=Y_test
plt.scatter(predictions, actual_values, alpha=.75, color='b')
plt.xlabel('Predicted Price')
plt.ylabel('Actual Price')
plt.title('Linear Regresion Model')
plt.show()

#Trying to imporve the model
#using Ridge Regulariztion, i.e. a process that shrinks the regression coefficients of the less important features
#Ridge Regularization takes a parameter, alpha, which controls the strength pf the regularization 
#alpha is a hyper parameter
#loop through a few different values of alpha

for i in range (-2,3):
    alpha=10**1
    rn=linear_model.Ridge(alpha=alpha)
    ridge_model=rn.fit(X_train, Y_train)
    preda_ridge=ridge_model.predict(X_test)
    
    plt.scatter(preda_ridge, actual_values, alpha=.75, color='b')
    plt.xlabel('Predicted Price')
    plt.ylabel('Actual Price')
    plt.title('Ridge Regularization with alpha =()' .format(alpha))
    overlay='R^2 is: ()\n RMSE is: ()'.format(ridge_model.score(X_test, Y_test), mean_squared_error(Y_test, preda_ridge))
    plt.annotate(s=overlay, xy=(12.1,10.6), size='x-large')
    plt.show()
    
#Visualize results
submission=pd.DataFrame()
submission['Id']=test_df.Id
#Select features from test data for model
feats=test_df.select_dtypes(include=[np.number]).drop(['Id'], axis=1).interpolate()

predictions=model.predict(feats)
#Converted features to log form, reversing that step
final_predictions=np.exp(predictions)  

#to see difference;
print("Original predictions are: \n", predictions[:10], "\n")
print("Final predictions are: \n", final_predictions[:10], "\n")
  
submission['SalePrice']=final_predictions 

print(submission.head())

submission.to_csv('Submission.csv', index=False)




